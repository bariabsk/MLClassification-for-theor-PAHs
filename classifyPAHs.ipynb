{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b1567f0-2696-418e-8828-cd3f11ebf61b",
   "metadata": {},
   "source": [
    "#### Import necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a41a06-4de8-4d83-bfd2-972c7c76661f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas:\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "from time import time\n",
    "\n",
    "#Numpy, matplotlib & astropy:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import random\n",
    "from astropy.table import Table\n",
    "import PlottingStyle #Author-defined \n",
    "\n",
    "#PAHdbPythonSuite:\n",
    "import pkg_resources\n",
    "from pkg_resources import resource_filename\n",
    "from AmesPAHdbPythonSuite.amespahdbpythonsuite.amespahdb import AmesPAHdb\n",
    "from AmesPAHdbPythonSuite.amespahdbpythonsuite.xmlparser import XMLparser\n",
    "from AmesPAHdbPythonSuite.amespahdbpythonsuite import observation\n",
    "\n",
    "\n",
    "#Import modules from scikit-learn:\n",
    "\n",
    "# For data scaling:\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "# Pipeline, Grid Search, train_test_split, Cross-Validation: \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_validate, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "#Classifiers & Metrics:\n",
    "from sklearn.metrics import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "#Imbalanced-learn library:\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca71a4d-993a-40e7-9a46-77b0b8424027",
   "metadata": {},
   "source": [
    ">## <span style='color:DarkBlue'> I. Data Collection:  </span> Importing the PAHdb theoretical database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48923d-92f3-4667-be74-80d6418362b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to database to be read using the amespahdbpythonsuite package:\n",
    "\n",
    "path = '../../pahdb-complete-theoretical-v3.20_X3N8UL.xml'\n",
    "\n",
    "#XML Parser:\n",
    "xml = pkg_resources.resource_filename('amespahdbpythonsuite', path)\n",
    "parser = XMLparser(xml)\n",
    "parser.verify_schema()\n",
    "library = parser.to_pahdb_dict()\n",
    "\n",
    "\n",
    "# Parsing the database using the AmesPAHdbPythonSuite:\n",
    "db = AmesPAHdb(filename=resource_filename('amespahdbpythonsuite', path),\n",
    "                  check=False, cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a43d69-3051-4d34-837a-973bb42f2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information available:\n",
    "library['species'][430].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea908dfd-f673-43eb-884d-90ddc8073fc2",
   "metadata": {},
   "source": [
    ">## <span style='color:DarkBlue'> II. Data Preparation:  </span> Extracting relevant features for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3b867-fcff-4fd3-b897-2c067225e9ff",
   "metadata": {},
   "source": [
    "Selection criteria: Selecting \"astronomically relevant\" PAH species i..e those that have atleast 20 Carbon atoms and are either a \"pure\" PAH or <br> PANH and have neither any aliphatic side group groups nor O, Mg, Si or Fe atoms. The final dataframe built has the following features: <br>\n",
    "\n",
    "Specie UID, Charge, Charge State, Size (No. of C atoms), solos, duos, trios, quartets, $s/N_{H}$, $d/N_H$, $t/N_H$, $q/N_H$, $A_{11.2}$, $A_{12.0}$, $A_{12.7}$, $A_{13.5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d80e83-2fa1-4f9c-8e01-f2d0d4d305fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique identifiers of PAH species that satisy the selection criteria are fetched using the database parser: \n",
    "specie_uids = db.search(\"h>0 c>20 ch2=0 chx=0 oxygen=0 magnesium=0 silicium=0 iron=0\")\n",
    "\n",
    "#Build the data catalog: \n",
    "\n",
    "#Charges:\n",
    "charges  = [library['species'][specie_uid]['charge'] for specie_uid in specie_uids]\n",
    "\n",
    "#No. of C atoms indicating size:\n",
    "n_C = [float(library['species'][specie_uid]['formula'][1:library['species'][specie_uid]['formula'].index('H')]) for specie_uid in specie_uids]\n",
    "\n",
    "#Electronic Transitions - information from stick spectra:\n",
    "transitions  = [library['species'][specie_uid]['transitions'] for specie_uid in specie_uids]\n",
    "frequencies = [[i['frequency'] for i in specie_trans] for specie_trans in transitions] #[1/cm]\n",
    "intensities = np.array([np.array([(i['intensity']) for i in specie_trans]) for specie_trans in transitions], dtype=object) #[km / mol]\n",
    "wavelengths = [(1/(np.array(frequency_lst) + 15.)) * 10000 for frequency_lst in frequencies] #from [cm^-1 to [μm]. \n",
    "#A redshift of 15 cm^-1 has been applied to account for the wavelength shift between emission and absorption process.\n",
    "#Discussed in Section 3 in Bauschlicher, Peeters, Allamandola (2009).\n",
    "\n",
    "#No. of Solo, duo, trio or quartet groups in the molecules:\n",
    "n_solo = np.array([library['species'][specie_uid]['n_solo']/1. for specie_uid in specie_uids])\n",
    "n_duo = np.array([library['species'][specie_uid]['n_duo']/2. for specie_uid in specie_uids])\n",
    "n_trio = np.array([library['species'][specie_uid]['n_trio']/3. for specie_uid in specie_uids])\n",
    "n_quartet = np.array([library['species'][specie_uid]['n_quartet']/4. for specie_uid in specie_uids])\n",
    "n_quintet = np.array([library['species'][specie_uid]['n_quintet']/5. for specie_uid in specie_uids]) #Actually all zero..\n",
    "\n",
    "#Total no. of H atoms:\n",
    "n_H =n_solo + n_duo + n_trio + n_quartet + n_quintet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23115f92-1d60-40bb-9199-99f3bbb20f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Pandas dataframe with the necessary data:\n",
    "\n",
    "\n",
    "d = {'Specie_UID': specie_uids, 'Charge': charges, \"Charge state\": np.zeros(len(charges)), 'Size ($N_C$)': n_C, 'Wavelengths':wavelengths,\n",
    "     'A': intensities,\\\n",
    "     'solos': n_solo, 'duos': n_duo, 'trios': n_trio, 'quartets': n_quartet, \\\n",
    "     '$s/N_{H}$': n_solo/n_H, '$d/N_{H}$': n_duo/n_H,  '$t/N_{H}$': n_trio/n_H,\n",
    "     '$q/N_{H}$': n_quartet/n_H}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "#Assigning anion/cation/neutral designations:\n",
    "\n",
    "df.loc[df.Charge == 0, 'Charge state'] = 'Neutral'\n",
    "df.loc[df.Charge > 0, 'Charge state'] = 'Cation'\n",
    "df.loc[df.Charge < 0, 'Charge state'] = 'Anion'\n",
    "\n",
    "pd.set_option('display.max_colwidth', 10)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86c281-3ae8-497e-b68b-54fe17ab9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncating the wavelengths of transitions to the 10-15μm range:\n",
    "\n",
    "for mol in range(len(df)):\n",
    "    wave_10_15_micron = []\n",
    "    abs_10_15_micron = []\n",
    "\n",
    "    for wave,abs_coeff in zip(df.iloc[mol]['Wavelengths'],df.iloc[mol]['A']):\n",
    "        if wave > 10. and wave < 15.:\n",
    "            wave_10_15_micron.append(wave)\n",
    "            abs_10_15_micron.append(abs_coeff)\n",
    "            \n",
    "    if len(wave_10_15_micron) > 0:\n",
    "        df.at[mol,'Wavelengths'] = np.sort(wave_10_15_micron)\n",
    "        df.at[mol,'A'] = np.array(abs_10_15_micron)[np.argsort(wave_10_15_micron)]\n",
    "\n",
    "        \n",
    "    else:\n",
    "        df.at[mol,'Wavelengths'] = np.nan\n",
    "        df.at[mol,'A'] = np.nan\n",
    "\n",
    "        #It was later confirmed that each molecule had transitions in the 10-15μm eg. through: df[df.isna().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a40bc-8723-4c9d-a2fe-bbbb45eb38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding columns of A-values [related to average absorption cross-sections] in four wavelength regions:\n",
    "\n",
    "df[\"$A_{11.2}$\"],df[\"$A_{12.0}$\"], df[\"$A_{12.7}$\"], df[\"$A_{13.5}$\"] = [[np.nan]*len(df) for i in range(4)]\n",
    "\n",
    "## Determining average A-values at particular wavelength regions and adding to the dataframe:\n",
    "\n",
    "wave_reg1 = [10.6, 11.4]\n",
    "wave_reg2 = [11.35, 12.8]\n",
    "wave_reg3 = [12.5, 13.3]\n",
    "wave_reg4 = [13.0, 13.9]\n",
    "\n",
    "for mol in range(len(df)):\n",
    "    s1,s2,s3,s4 =  [0 for i in range(4)]\n",
    "\n",
    "    waves = df.iloc[mol]['Wavelengths']\n",
    "    A_vals = df.iloc[mol]['A']\n",
    " \n",
    "    for i in range(len(waves)):\n",
    "        wave = waves[i]\n",
    "        A_val = A_vals[i]\n",
    "        if wave >= wave_reg1[0] and wave <= wave_reg1[1]:\n",
    "            s1 += A_val\n",
    "        elif wave >= wave_reg2[0] and wave <= wave_reg2[1]:\n",
    "            s2 += A_val\n",
    "        elif wave >= wave_reg3[0] and wave <= wave_reg3[1]:\n",
    "            s3 += A_val\n",
    "        elif wave >= wave_reg4[0] and wave <= wave_reg4[1]:\n",
    "            s4 += A_val\n",
    "        \n",
    "    if s1 != 0.:\n",
    "        df.at[mol,\"$A_{11.2}$\"] = s1\n",
    "    if s2 != 0.:\n",
    "        df.at[mol,'$A_{12.0}$'] = s2\n",
    "    if s3 != 0.:\n",
    "        df.at[mol,'$A_{12.7}$'] = s3\n",
    "    if s4 != 0.:\n",
    "        df.at[mol,'$A_{13.5}$'] = s4\n",
    "\n",
    "#Drop rows with NaN values:\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3192d-6174-475a-a191-84dbfebd63db",
   "metadata": {},
   "source": [
    ">## <span style='color:DarkBlue'> III. Data Exploration:  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b501f5-7128-4cab-b0bd-e975c3d57733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of relevant features:\n",
    "\n",
    "cols = ['Charge', 'Size ($N_C$)', '$s/N_{H}$', '$d/N_{H}$', '$t/N_{H}$', '$q/N_{H}$',\\\n",
    "                  '$A_{11.2}$',  '$A_{12.0}$',  '$A_{12.7}$', '$A_{13.5}$']\n",
    "df.hist(grid = False, figsize = (10,10), histtype='step', hatch='///', color = 'darkblue', \n",
    "        column = cols);\n",
    "plt.savefig(\"Plots/DataStats_Hist.png\", dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ab2f9-5096-4af5-af19-552c5fc43deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A scatterplot of two quantitivae features:\n",
    "\n",
    "plt.scatter(x = df[df[\"Charge state\"] == 'Anion']['$A_{11.2}$'], y =  df[df[\"Charge state\"] == 'Anion']['$A_{12.7}$'], s =8, c='White',\n",
    "         edgecolors='red', label = \"Anions\")\n",
    "plt.scatter(x = df[df[\"Charge state\"] == 'Cation']['$A_{11.2}$'], y =  df[df[\"Charge state\"] == 'Cation']['$A_{12.7}$'], s =8, label = \"Cations\", c='White',\n",
    "         edgecolors='blue')\n",
    "plt.scatter(x = df[df[\"Charge state\"] == 'Neutral']['$A_{11.2}$'], y =  df[df[\"Charge state\"] == 'Neutral']['$A_{12.7}$'], s =8, label = \"Neutrals\", c='White',\n",
    "         edgecolors='purple')\n",
    "plt.xlabel(\"11.2 micron\")\n",
    "plt.ylabel(\"12.7 micron\")\n",
    "plt.title(\"A-values for 11.2 $\\mu m$ vs. 12.7 $\\mu m$ emission\");\n",
    "plt.legend()\n",
    "plt.ylim(0,400); #Truncate axes for clear display\n",
    "plt.xlim(0,400);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd04554-b95e-43e4-af9c-54979997e5ab",
   "metadata": {},
   "source": [
    "No clear distinguishing boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6bd2a-6155-4a98-8bf4-efa261aea0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix:\n",
    "\n",
    "corr = df.corr(numeric_only = True)\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7d77d-70af-470d-ac4b-8087b3474c82",
   "metadata": {
    "tags": []
   },
   "source": [
    ">## <span style='color:DarkBlue'> IV. Data Preprocessing:  </span> For Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc831520-7aaa-4d05-a843-6d7df94961c9",
   "metadata": {},
   "source": [
    ">### <span style='color:Purple'> i.  Prepare dataframe of input features and output array of charge class labels:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440d9dc-ba1b-457a-a133-121f19aa460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing relevant subset of features from the full dataframe for ML workflow:\n",
    "\n",
    "df_input = df.iloc[:,[3,10,11,12,13,14,15,16,17]]\n",
    "df_input\n",
    "\n",
    "X = df_input\n",
    "y = df['Charge state'].values.flatten()\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00d203-714f-491a-8edf-980a98e968e0",
   "metadata": {},
   "source": [
    ">### <span style='color:Purple'> ii. Prepare Training and Test datasets:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479bab42-623c-446c-b642-3a97a7605a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine box plot of features:\n",
    "\n",
    "df.boxplot(column = cols);\n",
    "plt.xticks(rotation = 45);\n",
    "plt.savefig(\"Plots/DataStats_BoxPlot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401dfe8-5839-4671-8246-d872a8aec2e9",
   "metadata": {},
   "source": [
    "Certain features require transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff25590-6025-4431-8762-3b05a6cbb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X, y, test_size = 0.25, augment = False, random_state = 8):\n",
    "    \n",
    "    \"\"\" Split the dataframe of input features X and target labels y,\n",
    "    into random train and test subsets using train_test_split function from scitkit-learn's model selection class.\n",
    "    Default test_size is 0.25.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        X [pandas.DataFrame]: input data of the molecules\n",
    "        \n",
    "        y [pandas.DataFrame]: output data i.e. molecular charge state labels (\"Neutral\", \"Cation\" or \"Anion\")\n",
    "        \n",
    "        augment [boolean]: Specifies whether or not to augment the training data set \n",
    "        \n",
    "        random_state [int]: Controls the random number generator used for Scikit-learn algorithms. Set to 8.\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        X_train [pandas.DataFrame]: input data to train a classification model on \n",
    "        \n",
    "        X_test [pandas.DataFrame]: input data to test a classification model on \n",
    "        \n",
    "        y_train [numpy.array]: output data to train a classification model for \n",
    "        \n",
    "        y_test [numpy.array]: output data to test a classification model for \n",
    "        \n",
    "        classes [list]: lclass names corresponding to the encoded target values by a LabelEncoder transformer\n",
    "        \n",
    "        le : LabelEncoder object used to encode target values \n",
    "    \"\"\"\n",
    "    \n",
    "    #Split X and y into random train and test subsets:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    #Encode target labels [\"Anion\", \"Cation\", Neutral\"] with values 0,1,2, respectively. \n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    classes = le.classes_\n",
    "    \n",
    "    #If augmenting data is true, then augment the training data. Here oversampling using SMOTE (Synthetic Minority Oversampling TEchnique) \n",
    "    #and undersampling using ENN (Edited Nearest Neighbours) is accomplished through using the SMOTEENN function from the Imbalanced-learn library, \n",
    "    #which is based on scikit-learn.\n",
    "    \n",
    "    if augment == True:\n",
    "        \n",
    "        sme = SMOTEENN(random_state=random_state)\n",
    "        X_train, y_train = sme.fit_resample(X_train, y_train)\n",
    "\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, classes, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92a5b5-fc5c-48f1-937e-c008b3da1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test sets:\n",
    " \n",
    "# Without augmenting:\n",
    "X_train, X_test, y_train, y_test, classes, le  = get_train_test(X, y, augment = False)\n",
    "\n",
    "# With augmenting:\n",
    "X_train_aug, X_test_aug, y_train_aug, y_test_aug, classes, le  = get_train_test(X, y, augment = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95742e5c-ce6f-47f6-b26a-ddc83be61b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count class proportions in unaugmented and augmented training datasets:\n",
    "\n",
    "df_count = pd.DataFrame({'Charge State':['Neutrals', 'Anions', 'Cations'],\n",
    "                         'Count': [len(y_train[y_train == 2]),len(y_train[y_train == 0]),len(y_train[y_train == 1])]})\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df93bb3-34e5-4948-81cb-c8ce19393db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_aug = pd.DataFrame({'Charge State':['Neutrals', 'Anions', 'Cations'],\n",
    "                         'Count': [len(y_train_aug[y_train_aug == 2]),len(y_train_aug[y_train_aug == 0]),len(y_train_aug[y_train_aug == 1])]})\n",
    "df_count_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86810f9-6f28-4a11-959f-59999d1068f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pie-charts showing proportions of the three classes, before and after augmenting the training dataset. \n",
    "\n",
    "random.seed(40)\n",
    "df_count.groupby(['Charge State']).sum().plot(kind='pie', y='Count',colors = random.sample(get_cmap('twilight').colors, k=3), \n",
    "                                             autopct='%.1f\\%%', legend=None, pctdistance=0.4, labeldistance = .6,\n",
    "                                             textprops={'color':\"w\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d4e4c-3675-4d48-9c96-52e9fd324f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(40)\n",
    "df_count_aug.groupby(['Charge State']).sum().plot(kind='pie', y='Count',colors = random.sample(get_cmap('twilight').colors, k=3), \n",
    "                                             autopct='%.1f\\%%', legend=None, pctdistance=0.4, labeldistance = .6,\n",
    "                                             textprops={'color':\"w\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50bac4-dd99-4172-b0d3-8066d09c24a5",
   "metadata": {},
   "source": [
    ">### <span style='color:Purple'> iii. Feature Transformations:</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8e6fd-c5a9-41c7-81a3-c184eb3e3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform selected input features:\n",
    "\n",
    "def get_transformer():\n",
    "    \n",
    "    \"\"\"Build transformers to transform distributions of certain features to be more normal-like, for better performance of the \n",
    "        classification algorithms: \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        None\n",
    "       \n",
    "    Returns:\n",
    "    -------\n",
    "        transformer: A ColumnTransformer object that allows different column subsets of the input data to be transformed separately. \n",
    "        Consists of fitted transformers as tuples. \n",
    "        \n",
    "        to_scale [list]: features to be transformed using Standard Scaling. \n",
    "        \n",
    "        to_log [list]: features to be transformed using Power Scaling. \n",
    "    \"\"\"\n",
    "\n",
    "    #Apply a power transform featurewise to make data more Gaussian-like.\n",
    "    to_log = [\"Size ($N_C$)\", '$A_{11.2}$',  '$A_{12.0}$',  '$A_{12.7}$', \"$A_{13.5}$\"]\n",
    "    \n",
    "    #Standardize:\n",
    "    to_scale = ['$s/N_{H}$', '$d/N_{H}$', '$t/N_{H}$', '$q/N_{H}$'] \n",
    "\n",
    "    #Make pipelines of transformers:\n",
    "    scale_pipe = make_pipeline(StandardScaler())\n",
    "    log_pipe = make_pipeline(PowerTransformer())\n",
    "\n",
    "    #Full processor:\n",
    "    transformer = ColumnTransformer(\n",
    "                transformers=[\n",
    "                            (\"scale\", scale_pipe, to_scale),\n",
    "                            (\"log_transform\", log_pipe, to_log)\n",
    "    ])\n",
    "    \n",
    "    return transformer, to_scale, to_log\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0e8ed-9dd5-4ebc-a249-c92c8f714f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine box plot of features after transformation:\n",
    "\n",
    "X_transformed= pd.DataFrame(get_transformer()[0].fit_transform(X), columns=cols[1:])\n",
    "X_transformed.boxplot(column = cols[1:]);\n",
    "plt.xticks(rotation = 45);\n",
    "plt.savefig(\"Plots/DataStats_Transformed_BoxPlot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fb408-63c6-4a2d-ad41-69c14a844217",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pre and post-transformation plots on augmented train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412f1e9-3feb-402e-93f7-ca6b8d548426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Transform features in augmented training dataset and examine distributions:\n",
    "\n",
    "X_train_transformed= get_transformer()[0].fit_transform(X_train_aug)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 6))\n",
    "alpha = .3\n",
    "hatch = \".....\"\n",
    "\n",
    "to_scale = get_transformer()[1]\n",
    "to_log = get_transformer()[2]\n",
    "\n",
    "random.seed(0)\n",
    "cols1 = random.sample(get_cmap('Dark2').colors, k=4)\n",
    "cols2 = random.sample(get_cmap('tab20b').colors, k=5)\n",
    "\n",
    "for (column, col, label) in zip([1,2,3,4],cols1, to_scale):\n",
    "    axs[0].hist(X_train_aug.iloc[:,column],  alpha=alpha,  color = col, bins = 5, hatch = hatch)\n",
    "\n",
    "for (column, col, label) in zip([1,2,3,4],cols1, to_scale):\n",
    "    axs[1].hist(X_train_transformed[:,column],  label = label, alpha=alpha, color = col, bins = 5, hatch = hatch)\n",
    "    axs[1].legend(frameon=False, loc = \"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b99b76-fca0-4cbc-a916-ce443d42491f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(15, 6))\n",
    "for (column, col, label) in zip([0,5,6,7,8],cols2, to_log):\n",
    "    axs[0].hist(X_train_aug.iloc[:,column],  alpha=alpha, color = col, bins = 5,  hatch = hatch)\n",
    "    axs[0].set_ylim(0,20)\n",
    "    \n",
    "for (column, col, label) in zip([0,5,6,7,8],cols2, to_log):\n",
    "    axs[1].hist(X_train_transformed[:,column],  alpha=alpha,  label = label, color = col, bins = 5,  hatch = hatch)\n",
    "    axs[1].legend(frameon=False, loc = \"lower right\")\n",
    "    axs[1].set_ylim(0,2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a9cebb-2c7f-4bcc-a678-a12d7ac97dee",
   "metadata": {},
   "source": [
    ">## <span style='color:DarkBlue'> V. Model Selection:  </span> Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f7a9f-192c-48fa-9eb8-1edfbeff231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(clf, X_train, y_train, transformer = get_transformer()[0], random_state=8):\n",
    "    \n",
    "    \"\"\"Construct a classification model and cross-validate it to estimate its effectiveness. \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "       clf [class]: an sklearn classifier\n",
    "       \n",
    "       X_train [pandas.DataFrame]: input data to train a classification model on \n",
    "       \n",
    "       y_train [pandas.DataFrame]: output data to train a classification model for \n",
    "               \n",
    "       transformer: ColumnTransformer object returned by get_transformer() function\n",
    "       \n",
    "       random_state [int]: Controls the random number generator used for Scikit-learn algorithms. Set to 8.\n",
    "\n",
    "       \n",
    "    Returns:\n",
    "    -------\n",
    "        pipeline [class]: Pipeline of transforms with a final estimator\n",
    "        \n",
    "        model [class]: Estimator / model\n",
    "        \n",
    "        cv_scores [dict]:  Array of scores of the estimator for each run of the cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    #Build a pipeline with the preprocessor and model:\n",
    "    pipeline = Pipeline(steps=\n",
    "                           [('preprocessor', transformer),\n",
    "                           ('clf', clf)])\n",
    "                        \n",
    "    #Fit the pipeline to train the model on the training set:\n",
    "    model = pipeline.fit(X_train, (y_train))\n",
    "                        \n",
    "    #Evaluate metrics by cross-validation on the model:\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=random_state)\n",
    "                        \n",
    "    cv_scores = cross_validate(estimator=model,\n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               cv=cv,\n",
    "                               scoring = [\"balanced_accuracy\", \"f1_weighted\"],\n",
    "               return_train_score=True, return_estimator = False)\n",
    "    \n",
    "    return pipeline, model, cv_scores\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d8c41-1b3a-4880-b651-78e13f9d2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CV_results():\n",
    "    \n",
    "    \"\"\" Construct various classifiers and plot cross-validation scores for comparison to aid with model selection. \n",
    "    \"\"\"\n",
    "    \n",
    "    #Obtain train and set data sets without any augmentation to the training set. \n",
    "    X_train, X_test, y_train, y_test, classes, le  = get_train_test(X, y, augment = False)\n",
    "\n",
    "    #Logistic Regression:\n",
    "    lg_pipeline, lg_clf, lg_scores = build_classifier(LogisticRegression(multi_class='multinomial', solver='lbfgs', class_weight = 'balanced'),\n",
    "                                        X_train = X_train,\n",
    "                                        y_train = y_train)\n",
    "        \n",
    "    #Support Vector Machine:\n",
    "    svm_pipeline, svm_clf, svm_scores = build_classifier(SVC(kernel ='rbf', decision_function_shape='ovr', class_weight = 'balanced'),\n",
    "                                          X_train = X_train,\n",
    "                                          y_train = y_train)\n",
    "    \n",
    "    #Decision Tree:\n",
    "    dt_pipeline, dt_clf, dt_scores = build_classifier(DecisionTreeClassifier(class_weight = 'balanced'),\n",
    "                                         X_train = X_train,\n",
    "                                          y_train = y_train)\n",
    "    \n",
    "    \n",
    "    #Random Forest Classifier:\n",
    "    rf_pipeline, rf_clf, rf_scores = build_classifier(RandomForestClassifier(class_weight = 'balanced'),\n",
    "                                         X_train = X_train,\n",
    "                                          y_train = y_train)\n",
    "    \n",
    "\n",
    "    #Gradient Boosting Classifier:\n",
    "    gb_pipeline, gb_clf, gb_scores = build_classifier(GradientBoostingClassifier(), \n",
    "                                         X_train = X_train,\n",
    "                                          y_train = y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    classifiers = ('Logistic Regression', 'SVM', 'Decision Tree', 'Random Forest', 'Gradient Boosting')\n",
    "    cv_scores = ['Balanced Accuracy (Train)', 'Balanced Accuracy (Test)', 'Weighted F1 Score (Train)', 'Weighted F1 Score (Test)']\n",
    "    \n",
    "    #Store mean and standard deviation of cross-validation scores:\n",
    "    mean_scores, std_scores = [],[]\n",
    "    \n",
    "    for scores in [lg_scores, svm_scores, dt_scores, rf_scores,gb_scores]:\n",
    "        mean_scores.append([np.mean(scores['train_balanced_accuracy']),\n",
    "                             np.mean(scores['test_balanced_accuracy']),\n",
    "                             np.mean(scores['train_f1_weighted']),\n",
    "                             np.mean(scores['test_f1_weighted'])])\n",
    "        std_scores.append([np.std(scores['train_balanced_accuracy']),\n",
    "                             np.std(scores['test_balanced_accuracy']),\n",
    "                             np.std(scores['train_f1_weighted']),\n",
    "                             np.std(scores['test_f1_weighted'])])\n",
    "        \n",
    "    mean_scores = np.array(mean_scores)\n",
    "    std_scores = np.array(std_scores)\n",
    "    \n",
    "    #Data frame of cross-validation scores:\n",
    "    df_means = pd.DataFrame(mean_scores, columns=cv_scores, index=classifiers)\n",
    "    df_err =  pd.DataFrame(std_scores, columns=cv_scores, index=classifiers)\n",
    "    \n",
    "    #Present results in a bar plot:\n",
    "    fig, ax = plt.subplots( figsize =(10,6))\n",
    "    df_means.plot.bar(color = plt.cm.twilight(np.linspace(0, 1, len(classifiers))), alpha = 0.8, yerr = df_err, capsize=2, ax=ax)\n",
    "    plt.legend(loc=\"center right\")\n",
    "    plt.xlim(right=7.5)\n",
    "    plt.tick_params(\n",
    "    axis='both',          \n",
    "    which='both',      \n",
    "    bottom=True,    \n",
    "    top=False,  \n",
    "    right = False,\n",
    "    labelbottom=True) \n",
    "    ax.tick_params(axis='x', labelrotation=30)\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(\"Plots/CVScores_comparison.pdf\", dpi = 200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30840545-d78a-4c3e-a611-5da1174e2b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CV_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3409557f-a4f2-447e-b30a-5f59d2ff5367",
   "metadata": {},
   "source": [
    "Go with a Random Forest Classifier for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb29a09-1fc8-46b9-88c8-8a423e6cebc3",
   "metadata": {},
   "source": [
    ">## <span style='color:DarkBlue'> VI. Model Selection:  </span> Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803a75f-1e1c-42ed-9c1d-5404b2a32bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_RandomForest():\n",
    "    \n",
    "    \"\"\" Tune the hyperparameters of a Random Forest Classifier by performing an exhaustive cross-validated grid-search over specified \n",
    "     parameter values by implementing sklearn's GridSearchCV scheme. \n",
    "     \n",
    "     \n",
    "     Parameters:\n",
    "     -------\n",
    "     None\n",
    "     \n",
    "     Returns:\n",
    "     -------\n",
    "     GridSearchCV [class]: Result of the grid search, containing the best classifier and its parameters. \n",
    "     \n",
    "     \"\"\"\n",
    "    #Build a Random Forest Classifier:\n",
    "    \n",
    "    X_train_aug, X_test_aug, y_train_aug, y_test_aug, classes, le  = get_train_test(X, y, augment = True)\n",
    "    \n",
    "    rf_pipeline_aug, rf_clf_aug, rf_scores_aug = build_classifier(RandomForestClassifier(),\n",
    "                                                                  X_train = X_train_aug,\n",
    "                                                                  y_train = y_train_aug)\n",
    "    \n",
    "    \n",
    "    #Tune hyperparameters the RF Classifier through a Grid Search:\n",
    "        \n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100 , stop = 500, num = 10)] # returns 10 numbers \n",
    "\n",
    "    max_features = ['sqrt', 'log2', None]\n",
    "\n",
    "    max_depth = [int(x) for x in np.linspace(10, 100, num = 10)] \n",
    "\n",
    "    max_depth.append(None)\n",
    "\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    criterion = [\"gini\", \"entropy\"]\n",
    "\n",
    "    \n",
    "    param_grid = {\"clf__n_estimators\": n_estimators,\n",
    "              \"clf__max_depth\": max_depth,\n",
    "              \"clf__max_features\": max_features,\n",
    "              \"clf__bootstrap\": [True, False],\n",
    "              \"clf__criterion\": [\"gini\", \"entropy\"]}\n",
    "        \n",
    "    \n",
    "    # Run Grid Search:\n",
    "    grid_search_aug = GridSearchCV(rf_pipeline_aug, param_grid=param_grid,\n",
    "                                   scoring=\"accuracy\",\n",
    "                                   n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    start = time()\n",
    "    grid_search_aug.fit(X_train_aug, y_train_aug)\n",
    "    print(\"GridSearchCV took %.2f seconds to complete.\" % ((time() - start)))\n",
    "    \n",
    "    #Results:\n",
    "    \n",
    "    print(\"**** Grid Search Results **** \\n\")\n",
    "\n",
    "    print(\"Best Model: {0}\".format(grid_search_aug.best_estimator_[\"clf\"]))\n",
    "    \n",
    "    return grid_search_aug\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c304b1d6-feca-482e-b658-30fdd2c24226",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_aug = tune_RandomForest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460efc4-e04b-4795-afc4-035468bb8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(grid_search_model):\n",
    "    \n",
    "    \"\"\" Evaluate performance of the best-tuned Random Forest Classifier by testing on the test data, and \n",
    "    inspecting the Classification Report, Confusion Matrix, and Receiver Operating Characteristic Curve. \n",
    "    The Feature Importance for the finalized model is also investigated.\n",
    "     \n",
    "     \n",
    "     Parameters:\n",
    "     -------\n",
    "     GridSearchCV [class]: Result of the grid search, containing the best classifier and its parameters. \n",
    "\n",
    "     \n",
    "     Returns:\n",
    "     -------\n",
    "     None\n",
    "     \n",
    "     \"\"\"\n",
    "    \n",
    "    \n",
    "    #Get predictions for test data using the tuned Random Forest Classifier:\n",
    "    RF_preds_aug = grid_search_aug.best_estimator_.predict(X_test_aug)\n",
    "    RF_probs_aug = grid_search_aug.best_estimator_.predict_proba(X_test_aug)\n",
    "    \n",
    "    \n",
    "    #Overall metrics:\n",
    "    \n",
    "    print(\"**** Metrics ****\\n\")\n",
    "    # Overall metrics\n",
    "    print(\"Test Accuracy:\",grid_search_aug.best_estimator_.score(X_test_aug, y_test_aug))\n",
    "    print(\"Overall Accuracy:\",accuracy_score(y_test_aug, RF_preds_aug))\n",
    "    print(\"Overall Precision:\",precision_score(y_test_aug, RF_preds_aug, average='weighted'))\n",
    "    print(\"Overall Recall:\",recall_score(y_test_aug, RF_preds_aug, average='weighted'))\n",
    "    print(\"F-Score: {0}\\n\".format(f1_score(y_test_aug, RF_preds_aug, average='weighted')))\n",
    "\n",
    "    #Classification Report:\n",
    "    print(\"**** Classification Report **** \\n\")\n",
    "    print(classification_report(y_test_aug, RF_preds_aug, target_names = le.classes_))\n",
    "    \n",
    "    #Confusion Matrix:\n",
    "    print(\"**** Confusion Marix **** \\n\")\n",
    "    cmp = ConfusionMatrixDisplay.from_estimator(grid_search_aug.best_estimator_, X = X_test_aug, y=y_test_aug,display_labels=le.classes_,\n",
    "                                            cmap = plt.cm.magma_r, normalize=\"true\")\n",
    "    plt.savefig(\"Plots/CM.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"**** ROC Curve **** \\n\")\n",
    "\n",
    "    #ROC (Receiver Operating Characteristic) Curve:\n",
    "    charge_classes = le.classes_\n",
    "          \n",
    "    # Get ROC metrics for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    thresh ={}\n",
    "    random.seed(40)\n",
    "    colors = random.sample(get_cmap('twilight').colors, k=3) \n",
    "\n",
    "    for i in range(len(charge_classes)):    \n",
    "        fpr[i], tpr[i], thresh[i] = roc_curve(y_test_aug, RF_probs_aug[:,i], pos_label=i)\n",
    "    \n",
    "    # Plot the ROC chart\n",
    "    plt.plot(fpr[0], tpr[0], label=le.classes_[0] + ' vs Rest', c = colors[0], lw = 2.)\n",
    "    plt.plot(fpr[1], tpr[1], label=le.classes_[1] + ' vs Rest',  c = colors[1], lw = 2.)\n",
    "    plt.plot(fpr[2], tpr[2], label=le.classes_[2] + ' vs Rest',  c = colors[2], lw = 2.)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best', frameon = False)\n",
    "    plt.savefig(\"Plots/ROCCurve.pdf\")\n",
    "\n",
    "    \n",
    "    macro_roc_auc_ovr = roc_auc_score(\n",
    "    y_test_aug,\n",
    "    RF_probs_aug,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"weighted\",\n",
    ")\n",
    "    \n",
    "    print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")\n",
    "    plt.show()\n",
    "     \n",
    "          \n",
    "    #Feature Importance:\n",
    "    feature_names = X.columns\n",
    "    perm_imp_rf_aug = permutation_importance(grid_search_aug.best_estimator_, X_test_aug, y_test_aug, n_repeats=10, random_state=8)\n",
    "    perm_forest_importances_aug = pd.Series(perm_imp_rf_aug.importances_mean, index=feature_names)\n",
    "          \n",
    "          \n",
    "    print(\"**** Feature Importance  **** \\n\")\n",
    "    fig, ax = plt.subplots()\n",
    "    perm_forest_importances_aug.plot.bar(yerr=perm_imp_rf_aug.importances_std, ax=ax, color = colors[1], capsize=2)\n",
    "    ax.set_ylabel(\"Mean Accuracy Decrease\")\n",
    "    fig.tight_layout()\n",
    "    plt.tick_params(\n",
    "        axis='both',          \n",
    "        which='both',      \n",
    "        bottom=True,    \n",
    "        top=False,  \n",
    "        right = False,\n",
    "        labelbottom=True) \n",
    "    \n",
    "    ax.tick_params(axis='x', labelrotation=30)\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "    plt.savefig(\"Plots/FeatureImp.pdf\", dpi = 200)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac8dc3-3acf-40b4-90f0-478a3bed7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(grid_search_aug)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
